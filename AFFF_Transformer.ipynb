{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nohyeong/PFAS_ML/blob/main/AFFF_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RTZrCpv2YFsf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "\n",
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "gyNu--CsvWHm"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, MolFromSmiles\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate, GlobalMaxPooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from transformers import AutoTokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "import hyperopt\n",
        "from hyperopt import STATUS_OK\n",
        "\n",
        "\n",
        "def positional_encoding(length, depth):\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)\n",
        "  angle_rads = positions * angle_rates\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1)\n",
        "\n",
        "  pos_encoding = pos_encoding[:, :depth]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "    self.pos_encoding = positional_encoding(length=d_model, depth=d_model)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = tf.expand_dims(x, -1)\n",
        "    x = self.dense(x)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[:length, :]\n",
        "    return x\n",
        "\n",
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "class GlobalSelfAttention(BaseAttention):\n",
        "    def call(self, x):\n",
        "        attn_output = self.mha(\n",
        "            query=x,\n",
        "            value=x,\n",
        "            key=x)\n",
        "        x = self.add([x, attn_output])\n",
        "        x = self.layernorm(x)\n",
        "        return x\n",
        "\n",
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, FF_num_layers, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential()\n",
        "    for _ in range(int(FF_num_layers)):\n",
        "        self.seq.add(tf.keras.layers.Dense(dff, activation='gelu'))\n",
        "        self.seq.add(tf.keras.layers.Dropout(dropout_rate))\n",
        "        dff = dff // 2\n",
        "    self.seq.add(tf.keras.layers.Dense(d_model))\n",
        "    self.linear = tf.keras.layers.Dense(d_model)\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    seq_output = self.seq(x)\n",
        "    x = self.add([x, seq_output])\n",
        "    x = self.linear(x)\n",
        "    x = self.layer_norm(x)\n",
        "    return x\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff, dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(d_model=d_model, )\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.pos_embedding(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "    return x\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(1, activation='linear')\n",
        "\n",
        "  @tf.function(reduce_retracing=True)\n",
        "  def call(self, inputs):\n",
        "    x = inputs\n",
        "    x = self.encoder(x)\n",
        "    x = GlobalMaxPooling1D()(x)\n",
        "    pred = self.final_layer(x)\n",
        "    return pred\n",
        "\n",
        "def scale_inputs(df, columns):\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(df[columns])\n",
        "    return scaled_data, scaler\n",
        "\n",
        "def preprocess_smiles(smiles_list):\n",
        "    smiles_list = smiles_list.to_list()\n",
        "    d_model = 64\n",
        "\n",
        "    tokenizer = Tokenizer(char_level=True)\n",
        "    tokenizer.fit_on_texts(smiles_list)\n",
        "    sequences = tokenizer.texts_to_sequences(smiles_list)\n",
        "    max_length = max([len(seq) for seq in sequences])\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=56, padding='post')\n",
        "    return padded_sequences\n",
        "\n",
        "def hyp_opt(params):\n",
        "    print('hyp; params: ', params)\n",
        "    tf.get_logger().setLevel('ERROR')\n",
        "    num_heads = params['num_heads']\n",
        "    num_layers = params['num_layers']\n",
        "    learning_rate = params['learning_rate']\n",
        "    dff = params['dff']\n",
        "    dropout_rate = params['dropout_rate']\n",
        "    batch_size = params['batch_size']\n",
        "    epochs = params['epochs']\n",
        "    d_model = params['d_model']\n",
        "\n",
        "    mse_fold = []\n",
        "\n",
        "    X_train = train.copy()\n",
        "    y_train = train['Removal_rate']\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    encoded_molecular_data = preprocess_smiles(X_train['SMILES'])\n",
        "    encoded_molecular_data = pd.DataFrame(encoded_molecular_data)\n",
        "\n",
        "    X_train.reset_index(drop=True, inplace=True)\n",
        "    y_train.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    train_scaled_inputs, scaler = scale_inputs(train, input_vars)\n",
        "    X_train_concate = pd.concat([pd.DataFrame(train_scaled_inputs), encoded_molecular_data], axis=1)\n",
        "\n",
        "    splitter = GroupKFold(n_splits=5)\n",
        "\n",
        "    best_params = None\n",
        "    best_RMSE = float('inf')\n",
        "    best_MAE = float('inf')\n",
        "\n",
        "    cv_rmse = []\n",
        "    cv_mae = []\n",
        "\n",
        "    for train_index, val_index in splitter.split(X_train, y_train, groups=train['group']):\n",
        "\n",
        "\n",
        "        X_train_fold, X_val_fold = tf.gather(X_train_concate, train_index), tf.gather(X_train_concate, val_index)\n",
        "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "\n",
        "        inputs = Input(shape=(d_model,))\n",
        "\n",
        "        transformer = Transformer(num_layers=num_layers, d_model=d_model, num_heads=num_heads,\n",
        "                                  dff=dff, dropout_rate=dropout_rate)\n",
        "\n",
        "        outputs = transformer(inputs)\n",
        "\n",
        "        transformer.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
        "\n",
        "        history = transformer.fit(X_train_fold, y_train_fold,\n",
        "                                  validation_data=(X_val_fold, y_val_fold),\n",
        "                                  batch_size = batch_size,\n",
        "                                  epochs=epochs, verbose=0)\n",
        "\n",
        "        train_losses.append(history.history['loss'])\n",
        "        val_losses.append(history.history['val_loss'])\n",
        "\n",
        "        y_val_pred = transformer.predict(X_val_fold)\n",
        "        rmse = np.sqrt(mean_squared_error(y_val_fold, y_val_pred))\n",
        "        mae = mean_absolute_error(y_val_fold, y_val_pred)\n",
        "\n",
        "        cv_rmse.append(rmse)\n",
        "        cv_mae.append(mae)\n",
        "\n",
        "    rmse_score = np.mean(cv_rmse)\n",
        "    mae_score = np.mean(cv_mae)\n",
        "\n",
        "    avg_train_loss = np.mean(train_losses, axis=0)\n",
        "    avg_val_loss = np.mean(val_losses, axis=0)\n",
        "\n",
        "    index_of_smallest = np.argmin(avg_val_loss)\n",
        "\n",
        "    return {'loss': rmse_score, 'status': STATUS_OK, 'params': best_params}\n",
        "\n",
        "def preprocess_data(df):\n",
        "    df['SMILES'] = df['SMILES'].str.replace('[()=]', '', regex=True)\n",
        "    df['charge'] = df.apply(lambda row: calculate_charge(row['pH'], row['pKa'], is_acid=True), axis=1)\n",
        "    df['charge_product'] = (df['IEP'] - df['pH']) * df['charge']\n",
        "    df['Init_conc'] = df['Init_conc']/df['Mw']\n",
        "    df['group'] = df['ref_ID'].map(str) + df['PFAS']\n",
        "\n",
        "    return df\n",
        "\n",
        "def calculate_charge(pH, pKa, is_acid=True):\n",
        "    ratio = 10 ** (pH - pKa)\n",
        "    fraction_deprotonated = ratio / (1 + ratio)\n",
        "\n",
        "    if is_acid:\n",
        "        charge = -fraction_deprotonated\n",
        "    else:\n",
        "        charge = 1 - fraction_deprotonated\n",
        "\n",
        "    return charge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wNUpqjFVgQsY"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel('AFFF_data_final_20240308.xlsx', sheet_name='data')\n",
        "\n",
        "df.columns = ['ID', 'Membrane', 'MWCO', 'IEP',\n",
        "              'CA', 'PFAS', 'Mw', 'Atomic charge', 'SMILES',\n",
        "              'Size', 'log Kow', 'pKa', 'Init_conc',\n",
        "              'IS', 'Pres', 'pH', 'Removal_rate', 'ref_ID', 'ref']\n",
        "\n",
        "df = preprocess_data(df)\n",
        "df = df.dropna(subset=['IEP'])\n",
        "df.index = range(len(df))\n",
        "\n",
        "input_vars = ['MWCO', 'CA', 'Size', 'log Kow', 'Init_conc', 'Pres', 'IS', 'charge_product']\n",
        "\n",
        "train = df[df['ref_ID']!=1]\n",
        "test = df[df['ref_ID']==1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "RV56R1iiN9FH"
      },
      "outputs": [],
      "source": [
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "d_model = 64\n",
        "num_heads_values = [2, 4, 8, 16, 32]\n",
        "dff_values = [4, 8, 16, 32]\n",
        "num_layers_values = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "batch_size_values = [4, 8, 16, 32, 64]\n",
        "FF_num_layers_values = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "\n",
        "space = {'num_heads': hp.choice('num_heads', num_heads_values),\n",
        "         'num_layers': hp.choice('num_layers', num_layers_values),\n",
        "         'dff': hp.choice('dff', dff_values),\n",
        "         'dropout_rate': hp.uniform('dropout_rate', 0.0, 0.3),\n",
        "         'learning_rate': hp.loguniform('learning_rate', np.log(0.0001), np.log(0.1)),\n",
        "         'batch_size': hp.choice('batch_size', batch_size_values),\n",
        "         'epochs': hp.uniformint('epochs', 1, 500),\n",
        "         'FF_num_layers': hp.choice('FF_num_layers', FF_num_layers_values),\n",
        "         'd_model': d_model}\n",
        "\n",
        "trials = Trials()\n",
        "best_params = fmin(fn=hyp_opt, space=space, algo=tpe.suggest, max_evals=10, trials=trials)\n",
        "\n",
        "best_params['num_heads'] = num_heads_values[best_params['num_heads']]\n",
        "best_params['num_layers'] = num_layers_values[best_params['num_layers']]\n",
        "best_params['dff'] = dff_values[best_params['dff']]\n",
        "best_params['batch_size'] = batch_size_values[best_params['batch_size']]\n",
        "best_params['FF_num_layers'] = FF_num_layers_values[best_params['FF_num_layers']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2l83bPZXOQRh"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "encoded_molecular_data = pd.DataFrame(preprocess_smiles(train['SMILES']))\n",
        "d_model = 64\n",
        "\n",
        "train_scaled_inputs, scaler = scale_inputs(train, input_vars)\n",
        "X_train_concate = pd.concat([pd.DataFrame(train_scaled_inputs), encoded_molecular_data], axis=1)\n",
        "X_train_tf = tf.convert_to_tensor(X_train_concate)\n",
        "y_train = train['Removal_rate']\n",
        "\n",
        "inputs = Input(shape=(d_model,))\n",
        "\n",
        "transformer = Transformer(num_layers=best_params['num_layers'], d_model=d_model, num_heads=\n",
        "                          best_params['num_heads'], dff=best_params['dff'],\n",
        "                          dropout_rate=best_params['dropout_rate'])\n",
        "\n",
        "outputs = transformer(inputs)\n",
        "\n",
        "transformer.compile(optimizer=Adam(learning_rate=best_params['learning_rate']), loss='mse')\n",
        "\n",
        "history = transformer.fit(X_train_tf, y_train,\n",
        "                          batch_size = best_params['batch_size'],\n",
        "                          epochs=int(best_params['epochs']), verbose=1)\n",
        "\n",
        "X_test_scaled = scaler.fit_transform(test[input_vars])\n",
        "test_encoded_molecular_data = pd.DataFrame(preprocess_smiles(test['SMILES']))\n",
        "\n",
        "test.index = range(len(test))\n",
        "\n",
        "X_test_concate = pd.concat([pd.DataFrame(X_test_scaled), test_encoded_molecular_data], axis=1)\n",
        "X_test_tf = tf.convert_to_tensor(X_test_concate)\n",
        "y_test = test['Removal_rate']\n",
        "\n",
        "y_pred = transformer.predict(X_test_tf)\n",
        "test['y_pred'] = y_pred\n",
        "train_predictions = transformer.predict(X_train_tf)\n",
        "train['y_pred'] = train_predictions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "def plot_predictions(train, test, train_predictions):\n",
        "    train['mark'] = 'train'\n",
        "    test['mark'] = 'test'\n",
        "    total_data = pd.concat([train, test])\n",
        "\n",
        "    X_title = 'Real removal (%)'\n",
        "    Y_title = 'Pred removal (%)'\n",
        "    title_font_size = 43\n",
        "    tickfont_size = 35\n",
        "    ticklen = 5\n",
        "    tickwidth = 2\n",
        "    title_standoff = 20\n",
        "    plot_width = 600\n",
        "    plot_height = 500\n",
        "\n",
        "    train_pred_mae = mean_absolute_error(train_predictions, train['Removal_rate'])\n",
        "    train_pred_rmse = mean_squared_error(train_predictions, train['Removal_rate'], squared=False)\n",
        "\n",
        "    pred_mae = mean_absolute_error(test['y_pred'], test['Removal_rate'])\n",
        "    pred_rmse = mean_squared_error(test['y_pred'], test['Removal_rate'], squared=False)\n",
        "\n",
        "    print(f\"MAE: {np.round(pred_mae,2)}, RMSE: {np.round(pred_rmse,2)}\")\n",
        "    print(f\"Train MAE: {np.round(train_pred_mae,2)}, Train RMSE: {np.round(train_pred_rmse,2)}\")\n",
        "\n",
        "\n",
        "    fig = px.scatter(total_data, x='Removal_rate', y='y_pred', color='mark', width=plot_width, height=plot_height, range_x=[-10, 120], range_y=[-10, 120])\n",
        "\n",
        "    # Customize layout\n",
        "    fig.update_layout(\n",
        "        xaxis=dict(\n",
        "            dtick=25,\n",
        "            title_text=X_title,\n",
        "            title_font={\"size\": title_font_size},\n",
        "            tickfont=dict(family='Arial', color='black', size=tickfont_size),\n",
        "            title_standoff=title_standoff,\n",
        "            color='black',\n",
        "            ticks=\"inside\",\n",
        "            ticklen=ticklen,\n",
        "            tickwidth=tickwidth,\n",
        "            title_font_family=\"Arial\"),\n",
        "        yaxis=dict(\n",
        "            dtick=25,\n",
        "            title_text=Y_title,\n",
        "            title_font={\"size\": title_font_size},\n",
        "            tickfont=dict(family='Arial', color='black', size=tickfont_size),\n",
        "            title_standoff=title_standoff,\n",
        "            color='black',\n",
        "            ticks=\"inside\",\n",
        "            ticklen=ticklen,\n",
        "            tickwidth=tickwidth,\n",
        "            title_font_family=\"Arial\"))\n",
        "\n",
        "    fig.update_layout(plot_bgcolor='rgb(256,256,256)', showlegend=False)\n",
        "\n",
        "    fig.update_xaxes(showline=True, linewidth=4, linecolor='black', mirror=True)\n",
        "    fig.update_yaxes(showline=True, linewidth=4, linecolor='black', mirror=True)\n",
        "\n",
        "    random_x = [0, 100]\n",
        "    random_y0 = [0, 100]\n",
        "    fig.add_trace(go.Scatter(x=random_x, y=random_y0, line=dict(color='red', width=2), marker=dict(size=1)))\n",
        "\n",
        "    fig.update_traces(marker=dict(size=10, opacity=0.65, line=dict(width=0.7)), selector=dict(mode='markers'))\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "plot_predictions(train, test, train_predictions)"
      ],
      "metadata": {
        "id": "PYkKRLSE7YgQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1wF5v7mVCqY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}